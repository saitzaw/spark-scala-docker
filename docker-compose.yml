services:
  spark-master:
    container_name: spark-master
    build:
      context: .
      dockerfile: docker/spark.Dockerfile
    image: spark-image
    entrypoint: ['./entrypoint.sh', 'master']
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ./env/.env:/opt/spark/jobs/pyspark/src/.env
      - ./confs:/opt/spark/conf
      - ./spark-apps:/opt/spark/jobs
      - ./data:/opt/spark/data
      - ./spark-logs:/opt/spark/spark-events
    env_file:
      - env/.env.spark
    ports:
      - '8080:8080'
      - '7077:7077'

  spark-history-server:
    container_name: spark-history
    image: spark-image
    entrypoint: ['./entrypoint.sh', 'history']
    depends_on:
      - spark-master
    env_file:
      - env/.env.spark
    volumes:
      - ./env/.env:/opt/spark/jobs/pyspark/src/.env
      - ./confs:/opt/spark/conf
      - ./spark-apps:/opt/spark/jobs
      - ./data:/opt/spark/data
      - ./spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'

  spark-worker:
    container_name: spark-worker
    image: spark-image
    entrypoint: ['./entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    env_file:
      - env/.env.spark
    volumes:
      - ./env/.env:/opt/spark/jobs/pyspark/src/.env
      - ./confs:/opt/spark/conf
      - ./spark-apps:/opt/spark/jobs
      - ./data:/opt/spark/data
      - ./spark-logs:/opt/spark/spark-events
    ports:
      - '8081:8081'

  jupyter:
    container_name: spark-jupyter
    build:
      context: .
      dockerfile: docker/jupyter.Dockerfile
    ports:
      - "8888:8888"
    depends_on:
      - spark-master
      - postgres
    volumes:
      - ./notebooks:/opt/workspace
      - ./env/.env:/opt/workspace/.env
      - ./spark-logs:/opt/spark-events
    environment:
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=jupyter
      - PYSPARK_DRIVER_PYTHON_OPTS=notebook --no-browser --allow-root --ip=0.0.0.0 --port=8888
      - SPARK_MASTER=spark://spark-master:7077
    
  airflow-webserver:
    build:
      context: .
      dockerfile: docker/airflow.Dockerfile
    image: custom-airflow:2.9.2
    group_add:
    - ${DOCKER_GID}
    container_name: airflow-webserver
    depends_on:
      - postgres
      - redis
    env_file:
      - env/.env.airflow
    volumes:
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./spark-apps:/opt/spark/jobs
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - PYSPARK_PYTHON=python3
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - "8088:8080"
    command: webserver
    restart: always

  airflow-scheduler:
    image: custom-airflow:2.9.2
    group_add:
    - ${DOCKER_GID}
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - redis
    env_file:
      - env/.env.airflow
    volumes:
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./spark-apps:/opt/spark/jobs
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - PYSPARK_PYTHON=python3
      - SPARK_MASTER=spark://spark-master:7077
    command: scheduler
    restart: always

  airflow-worker:
    image: custom-airflow:2.9.2
    group_add:
    - ${DOCKER_GID}
    container_name: airflow-worker
    depends_on:
      - postgres
      - redis
    env_file:
      - env/.env.airflow
    volumes:
      - ./airflow.cfg:/opt/airflow/airflow.cfg
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./spark-apps:/opt/spark/jobs
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - PYSPARK_PYTHON=python3
      - SPARK_MASTER=spark://spark-master:7077
    command: celery worker
    restart: always

  redis:
    image: redis:latest
    container_name: airflow-redis
    ports:
      - "6379:6379"


  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,DOCKER://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
    volumes:
      - ./kafka-data:/var/lib/kafka/data

  schema-registry:
      image: confluentinc/cp-schema-registry:7.6.1
      container_name: schema-registry
      depends_on:
        - kafka
      ports:
        - "8085:8081"
      environment:
        SCHEMA_REGISTRY_HOST_NAME: schema-registry
        SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:29092
        SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8085


  control-center:
    image: confluentinc/cp-enterprise-control-center:7.6.1
    container_name: control-center
    depends_on:
      - kafka
      - schema-registry
      - zookeeper
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: kafka:29092
      CONTROL_CENTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085
      CONTROL_CENTER_CONNECT_CONNECT-DEFAULT_CLUSTER: http://debezium:8083
      CONTROL_CENTER_CONNECT_HEALTHCHECK_ENDPOINT: /connectors
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021

  debezium:
    image: debezium/connect:2.5
    container_name: debezium
    depends_on:
      - kafka
      - postgres
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: debezium-cluster
      CONFIG_STORAGE_TOPIC: debezium_config
      OFFSET_STORAGE_TOPIC: debezium_offsets
      STATUS_STORAGE_TOPIC: debezium_status
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085
      PLUGIN_PATH: /kafka/connect

  dbt:
    container_name: dbt
    user: "1000:1000"
    build:
      context: .
      dockerfile: docker/dbt.Dockerfile
    depends_on:
      - postgres
      - airflow-webserver
      - spark-master
    volumes:
      - ./sap_dbt:/usr/app
      - ./dags:/opt/airflow/dags         # allow airflow to see same DAGs
      - ./spark-apps:/opt/spark/jobs     # if using Spark
      - ./spark-logs:/opt/spark/spark-events
    working_dir: /usr/app
    environment:
      DBT_PROFILES_DIR: /usr/app/profiles
    command: tail -f /dev/null

  postgres:
    image: postgres:15
    container_name: postgres
    restart: always
    ports:
      - "5432:5432"
    env_file:
      - env/.env
    volumes:
      - ./pg_data:/var/lib/postgresql/data
      - ./init-sql:/sql_scripts

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # MinIO Console UI
    env_file:
      - env/.env.minio
    volumes:
      - ./lakehouse:/data

volumes:
  pg_data:
  spark-logs:
  airflow-logs:
  kafka-data:
  lakehouse: 